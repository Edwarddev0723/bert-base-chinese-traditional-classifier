# build_dataset.py
"""
Dataset Builder – configurable version
====================================
This refactor turns the original hard‑coded script into a *parameter‑driven* CLI tool.
You can override **every** important hyper‑parameter either:
  • via command‑line flags, **or**
  • by providing a YAML/JSON config file.

Typical usage
-------------
# simplest – rely on defaults
python build_dataset.py

# override a few knobs
python build_dataset.py \
    --rows 200000 \
    --out_dir data_cache_v2 \
    --min_zh_ratio 0.2

# drive everything from a config file
python build_dataset.py --config configs/cci3.yaml
"""

from __future__ import annotations

import argparse, json, os, re, random, hashlib, multiprocessing as mp, psutil, warnings
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Dict, Any

import yaml  # PyYAML
from datasets import load_dataset, Dataset, disable_caching, concatenate_datasets
from transformers import AutoTokenizer
from tqdm.auto import tqdm
from functools import lru_cache

# ---------------------------
# Config dataclass
# ---------------------------
@dataclass
class BuildCfg:
    seed: int = 42
    max_tok: int = 256
    buffer_size: int = 50_000
    batch_size: int = 5_000

    min_len: int = 10
    min_zh_ratio: float = 0.30

    rows: int = 150_000
    out_dir: str = "data_cache"
    out_name: str | None = None  # when None, autogenerated

    tokenizer_name: str = "ckiplab/bert-base-chinese"
    hf_endpoint: str = "https://hf-mirror.com"

    # each source = dict(dataset, split, mode, label, key)
    sources: List[Dict[str, Any]] | None = None  # populated below

    # cpu / memory‑aware worker count (computed later if -1)
    num_proc: int = -1

    def finalize(self):
        if self.sources is None:
            self.sources = [
                {"dataset": "voidful/fineweb-zhtw", "split": "train", "mode": "norm", "label": 1, "key": "text"},
                {"dataset": "austenjs/ClueCorpusSmallDataset", "split": "train", "mode": "simp2trad", "label": 0, "key": "text"},
            ]
        if self.num_proc == -1:
            self.num_proc = min(8, mp.cpu_count()) if psutil.virtual_memory().total/2**30 >= 16 else 4
        if self.out_name is None:
            self.out_name = f"fineweb_mix_{self.rows}.parquet"
        os.environ["HF_ENDPOINT"] = self.hf_endpoint
        disable_caching()
        random.seed(self.seed)
        warnings.filterwarnings("ignore", message="Token indices sequence length")

# ---------------------------
# Argument parsing helpers
# ---------------------------

def parse_args() -> BuildCfg:
    p = argparse.ArgumentParser(description="Configurable dataset builder (parquet)")
    p.add_argument("--config", type=str, help="YAML/JSON file with parameters", default=None)

    # frequently tweaked hyper‑params
    p.add_argument("--rows", type=int)
    p.add_argument("--out_dir", type=str)
    p.add_argument("--out_name", type=str)
    p.add_argument("--min_len", type=int)
    p.add_argument("--min_zh_ratio", type=float)
    p.add_argument("--max_tok", type=int)
    p.add_argument("--tokenizer_name", type=str)
    p.add_argument("--sources", type=str, help="Inline JSON list overriding sources")

    args = p.parse_args()

    cfg = BuildCfg()

    # external file overrides
    if args.config:
        with open(args.config, "r", encoding="utf-8") as fh:
            data = yaml.safe_load(fh) if args.config.endswith((".yaml", ".yml")) else json.load(fh)
        for k, v in data.items():
            if hasattr(cfg, k):
                setattr(cfg, k, v)

    # CLI > config > default
    for field in ("rows", "out_dir", "out_name", "min_len", "min_zh_ratio", "max_tok", "tokenizer_name"):
        val = getattr(args, field)
        if val is not None:
            setattr(cfg, field, val)

    if args.sources:
        cfg.sources = json.loads(args.sources)

    cfg.finalize()
    return cfg

# ---------------------------
# Regex & helpers
# ---------------------------
_RE_WEIRD = re.compile(r"[^a-zA-Z0-9\u4e00-\u9fff\s\.\,\!\?！？。，「」％%：:/／/~@#\(\)《》<>\-]")
_RE_ZH    = re.compile(r"[\u4e00-\u9fff]")
_SENT_BOUND = re.compile(r"([。！？；…]+)")

@lru_cache(maxsize=None)
def _cc(table: str):
    from opencc import OpenCC
    return OpenCC(table)

def _zh_ratio(t: str) -> float:
    return len(_RE_ZH.findall(t)) / max(len(t), 1)

def _clean(t: str) -> str:
    return _RE_WEIRD.sub('', t)

# ---------------------------
# Tokenizer bootstrap (lazy)
# ---------------------------
_tok = None

def tok(cfg: BuildCfg):
    global _tok
    if _tok is None:
        _tok = AutoTokenizer.from_pretrained(cfg.tokenizer_name)
    return _tok

# ---------------------------
# Sentence & chunk utilities
# ---------------------------

def sent_split(text: str) -> List[str]:
    parts, buf = [], []
    for seg in _SENT_BOUND.split(text):
        if seg:
            buf.append(seg)
            if _SENT_BOUND.fullmatch(seg):
                parts.append(''.join(buf).strip()); buf = []
    if buf:
        parts.append(''.join(buf).strip())
    return [s for s in parts if s]


def chunk_by_tokens(sentences: List[str], limit: int, cfg: BuildCfg) -> List[str]:
    chunks, cur, n = [], [], 0
    for s in sentences:
        l = len(tok(cfg)(s, add_special_tokens=False)["input_ids"])
        if n + l > limit and cur:
            chunks.append(''.join(cur)); cur, n = [], 0
        cur.append(s); n += l
    if cur:
        chunks.append(''.join(cur))
    return chunks

# ---------------------------
# Streaming + preprocessing
# ---------------------------

def stream_take(cfg: BuildCfg, src: Dict[str, Any], rows: int):
    name, split, key = src["dataset"], src["split"], src["key"]
    ds = load_dataset(name, split=split, streaming=True).shuffle(seed=cfg.seed, buffer_size=cfg.buffer_size)
    out, it = [], iter(ds)
    with tqdm(total=rows, desc=f"stream {name.split('/')[-1]}", ncols=80) as p:
        while len(out) < rows:
            ex = next(it)
            if ex.get(key):
                out.append({"text": ex[key]}); p.update()
    return Dataset.from_list(out)


def to_trad(texts: List[str], mode: str) -> List[str]:
    if mode == "simp2trad":
        texts = [_cc("s2t.json").convert(t) for t in texts]
    return [_cc("t2tw.json").convert(t).replace("\u3000", '').strip() for t in texts]


def prep(cfg: BuildCfg, src: Dict[str, Any], rows: int):
    ds = stream_take(cfg, src, rows)
    mode = src.get("mode", "norm")
    label = src.get("label", 0)

    # 1) 字形正規化
    ds = ds.map(lambda b: {"text": to_trad(b["text"], mode)}, batched=True, batch_size=cfg.batch_size, num_proc=cfg.num_proc)

    # 2) 分句 + token chunk
    # —————— 逐筆處理，避免 batched 模式產生列表長度不一致 ——————
    ds = ds.map(
        lambda ex: {"text": chunk_by_tokens(sent_split(ex["text"]), cfg.max_tok, cfg)},
        num_proc=cfg.num_proc,
    )
    ds = ds.flatten_indices()

    # 3) 清洗 + 篩選
    def _normalize_text(t):
        if isinstance(t, list):  # 還沒展平的遺漏 edge‑case
            t = "".join(t)
        return _clean(t)

    ds = ds.map(lambda ex: {"text": _normalize_text(ex["text"])} , num_proc=cfg.num_proc)
    ds = ds.filter(
        lambda ex: len(ex["text"]) >= cfg.min_len and _zh_ratio(ex["text"]) >= cfg.min_zh_ratio,
        num_proc=cfg.num_proc,
    )

    # 4) 標籤) 標籤 + 篩選
    ds = ds.map(lambda b: {"text": [_clean(t) for t in b["text"]]}, batched=True, batch_size=cfg.batch_size, num_proc=cfg.num_proc)
    ds = ds.filter(lambda ex: len(ex["text"]) >= cfg.min_len and _zh_ratio(ex["text"]) >= cfg.min_zh_ratio,
                   num_proc=cfg.num_proc)

    # 4) 標籤
    ds = ds.add_column("label", [label] * len(ds))
    return ds

# ---------------------------
# Deduplication helper
# ---------------------------

def dedup(ds: Dataset, cfg: BuildCfg):
    def add(batch):
        return {"key": [f"{hashlib.md5(t[:128].encode()).hexdigest()}_{lab}" for t, lab in zip(batch["text"], batch["label"])]}
    seen = set()
    def keep(batch):
        flags = []
        for k in batch["key"]:
            flags.append(k not in seen); seen.add(k)
        return {"keep": flags}

    ds = ds.map(add, batched=True, batch_size=cfg.batch_size, num_proc=cfg.num_proc)
    ds = ds.map(keep, batched=True, num_proc=1)
    ds = ds.filter(lambda ex: ex["keep"]).remove_columns(["key", "keep"])
    return ds.shuffle(seed=cfg.seed)

# ---------------------------
# Main build entry
# ---------------------------

def build(cfg: BuildCfg):
    each = cfg.rows // len(cfg.sources)
    parts = [prep(cfg, src, each) for src in cfg.sources]
    full = dedup(concatenate_datasets(parts), cfg)

    Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)
    out_path = Path(cfg.out_dir) / cfg.out_name
    full.to_parquet(out_path.as_posix(), compression="zstd")
    print("[✓] Saved ->", out_path)

# ---------------------------
# Entry point
# ---------------------------
if __name__ == "__main__":
    cfg = parse_args()
    print(json.dumps(asdict(cfg), ensure_ascii=False, indent=2))
    build(cfg)
    print("[✓] Dataset build completed successfully!")
    print(f"Output saved to: {Path(cfg.out_dir) / cfg.out_name}")