# tokenizer_util.py
"""
Tokenization & train/val split utilities
---------------------------------------
These helpers sit *after* the dataset-building phase (e.g. the Parquet
file generated by `build_dataset.py`).  They offer two main entry points:

1. `get_tokenizer()` – lazy‑loads and caches any HF tokenizer.
2. `split_and_tokenize()` – takes a `datasets.Dataset` (or DataFrame),
   stratified‑splits it, then *smart‑chunks* long texts with stride.

Example
~~~~~~~
>>> from datasets import Dataset
>>> from tokenizer_util import get_tokenizer, split_and_tokenize
>>> ds = Dataset.from_pandas(pd.read_parquet("fineweb.parquet"))
>>> enc, train_df, val_df = split_and_tokenize(ds, get_tokenizer())
"""

from __future__ import annotations

import os, multiprocessing as mp
from functools import lru_cache
from typing import Tuple, Union

import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from sklearn.model_selection import train_test_split

# ---------------------------------------------------------------------------
# Global config
# ---------------------------------------------------------------------------
MAX_SEQ_LEN  = 256
STRIDE       = 128
TEST_SIZE    = 0.2
RANDOM_STATE = 42
NUM_PROC     = mp.cpu_count() or 1

# ---------------------------------------------------------------------------
# Tokenizer bootstrap – cached to avoid re‑instantiation costs
# ---------------------------------------------------------------------------

@lru_cache(maxsize=None)
def get_tokenizer(model_name: str = "ckiplab/bert-base-chinese") -> PreTrainedTokenizerBase:
    """Lazily load and cache a HF tokenizer."""
    return AutoTokenizer.from_pretrained(model_name, use_fast=True)

# ---------------------------------------------------------------------------
# Core tokenization for a single example (handles overflow)
# ---------------------------------------------------------------------------

def _chunk_tokenize(text: str, label: int, tokenizer: PreTrainedTokenizerBase,
                    max_len: int = MAX_SEQ_LEN, stride: int = STRIDE):
    """Tokenize *one* text, returning lists of `input_ids`, `attention_mask`,
    and duplicated `label` entries – handling long docs via overflow chunks."""
    if len(text) <= max_len:
        out = tokenizer(text, truncation=True, max_length=max_len,
                         return_attention_mask=True, padding=False,
                         return_token_type_ids=False)
        return [out["input_ids"]], [out["attention_mask"]], [label]

    # long text – use sliding window
    out = tokenizer(text, truncation=True, max_length=max_len, stride=stride,
                    return_overflowing_tokens=True, return_attention_mask=True,
                    padding=False, return_token_type_ids=False)
    n_chunks = len(out["input_ids"])
    return out["input_ids"], out["attention_mask"], [label] * n_chunks

# ---------------------------------------------------------------------------
# Vectorised map fn for datasets.map (batched=False for safety)
# ---------------------------------------------------------------------------

def tokenize_example(example, tokenizer: PreTrainedTokenizerBase):
    ids, masks, labs = _chunk_tokenize(example["text"], example["label"], tokenizer)
    return {
        "input_ids": ids,
        "attention_mask": masks,
        "labels": labs,
    }

# ---------------------------------------------------------------------------
# Public helper – split, tokenize, return encoded DatasetDict + raw DataFrames
# ---------------------------------------------------------------------------

def split_and_tokenize(dataset: Union[Dataset, pd.DataFrame],
                        tokenizer: PreTrainedTokenizerBase | None = None,
                        test_size: float = TEST_SIZE,
                        random_state: int = RANDOM_STATE,
                        max_len: int = MAX_SEQ_LEN,
                        stride: int = STRIDE,
                        keep_raw: bool = True
                       ) -> Tuple[DatasetDict, pd.DataFrame | None, pd.DataFrame | None]:
    """Stratified split → tokenization → DatasetDict.

    Parameters
    ----------
    dataset : Dataset | DataFrame
        Source data with at least **text** and **label** columns.
    tokenizer : PreTrainedTokenizerBase, optional
        Custom tokenizer; defaults to `get_tokenizer()`.
    keep_raw : bool
        Whether to also return the raw train/val pandas DataFrames.
    """
    tok = tokenizer or get_tokenizer()

    # -------------------------------------------------------------------
    # Step 1 – to pandas then stratified split
    # -------------------------------------------------------------------
    if isinstance(dataset, Dataset):
        df = dataset.to_pandas()
    else:
        df = dataset.copy()

    train_df, val_df = train_test_split(
        df,
        test_size=test_size,
        stratify=df["label"],
        random_state=random_state,
    )

    raw_ds = DatasetDict({
        "train": Dataset.from_pandas(train_df.reset_index(drop=True)),
        "validation": Dataset.from_pandas(val_df.reset_index(drop=True)),
    })

    # -------------------------------------------------------------------
    # Step 2 – tokenize with multiprocessing (no batching for clarity)
    # -------------------------------------------------------------------
    encode = lambda ex: tokenize_example(ex, tok)
    encoded = raw_ds.map(
        encode,
        num_proc=NUM_PROC,
        remove_columns=[col for col in raw_ds["train"].column_names if col not in ("text", "label")],
        batched=False,
    )

    return (encoded, train_df, val_df) if keep_raw else (encoded, None, None)

# ---------------------------------------------------------------------------
# CLI quick‑test (python tokenizer_util.py parquet_path)
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    import sys
    from datasets import Dataset

    if len(sys.argv) < 2:
        raise SystemExit("Usage: python tokenizer_util.py <parquet_path>")

    parquet = sys.argv[1]
    ds = Dataset.from_pandas(pd.read_parquet(parquet))
    enc, train_df, val_df = split_and_tokenize(ds)

    print(enc)
    print("Train shape:", train_df.shape, "Val shape:", val_df.shape)
